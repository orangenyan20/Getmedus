import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import time
from io import BytesIO
from docx import Document
from docx.shared import Inches

# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆBOMé™¤å» + å¤šã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰
def try_read_file(uploaded_file):
    raw_bytes = uploaded_file.read()
    if raw_bytes.startswith(b'\xff\xfe'):
        encoding = 'utf-16'
    elif raw_bytes.startswith(b'\xfe\xff'):
        encoding = 'utf-16'
    elif raw_bytes.startswith(b'\xef\xbb\xbf'):
        encoding = 'utf-8-sig'
    else:
        encodings = ['utf-8', 'shift_jis', 'cp932', 'iso-2022-jp', 'utf-16', 'utf-16-le', 'utf-16-be']
        for enc in encodings:
            try:
                return [line.strip() for line in raw_bytes.decode(enc).splitlines() if line.strip()]
            except:
                continue
        st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return []
    try:
        return [line.strip() for line in raw_bytes.decode(encoding).splitlines() if line.strip()]
    except:
        st.error("âŒ BOMä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return []

# å•é¡Œç•ªå·ã‹ã‚‰URLç”Ÿæˆ
def generate_urls_from_ids(question_ids):
    base_url = "https://medu4.com/"
    return [f"{base_url}{qid.strip()}" for qid in question_ids if qid.strip()]

# ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ­£ç­”ç‡è¿½åŠ ï¼‰
def get_page_text(url, get_images=True):
    try:
        response = requests.get(url)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        category = soup.find('span', class_='button-small-line')
        category_name = category.text.strip() if category else 'åˆ†é‡åãªã—'

        problem = soup.find('div', class_='quiz-body mb-64')
        problem_text = problem.text.strip() if problem else 'å•é¡Œæ–‡ãªã—'

        choices = []
        for choice in soup.find_all('div', class_='box-select'):
            choice_header = choice.find('span', {'class': 'choice-header'}).text.strip()
            choice_text = choice.find_all('span')[1].text.strip()
            choices.append(f"{choice_header} {choice_text}")

        h4_tags = soup.find_all('h4')
        answer_text = 'è§£ç­”ãªã—'
        question_id = 'å•é¡Œç•ªå·ãªã—'
        if len(h4_tags) >= 2:
            answer_text = h4_tags[0].text.strip()
            question_id_match = re.search(r'([0-9]{3}[A-Za-z][0-9]+)', h4_tags[1].text)
            if question_id_match:
                question_id = question_id_match.group(1)

        explanation = soup.find('div', class_='explanation')
        explanation_text = explanation.text.strip() if explanation else 'è§£èª¬ãªã—'

        image_urls = []
        if get_images:
            image_divs = soup.find_all('div', class_='box-quiz-image mb-32')
            for div in image_divs:
                for img_tag in div.find_all('img'):
                    if img_tag.get('src'):
                        img_url = img_tag['src'].replace('thumb_', '')
                        image_urls.append(img_url)

        # æ­£ç­”ç‡
        accuracy = 'æ­£ç­”ç‡ä¸æ˜'
        for p in soup.find_all('p', class_='commentary-date'):
            match = re.search(r'æ­£ç­”ç‡ï¼š(\d+)%', p.text)
            if match:
                accuracy = int(match.group(1))
                break

        return {
            "category": category_name,
            "problem": problem_text,
            "choices": choices,
            "answer": answer_text,
            "question_id": question_id,
            "explanation": explanation_text,
            "images": image_urls,
            "accuracy": accuracy
        }
    except Exception as e:
        return None

# Wordãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
def create_word_doc(pages_data, search_query, include_images=True):
    doc = Document()
    doc.add_heading('æ¤œç´¢çµæœ', 0)
    doc.add_paragraph(f"å–å¾—å•é¡Œæ•°: {len(pages_data)}å•")

    for idx, page_data in enumerate(pages_data, start=1):
        title = f"å•é¡Œ{idx} {page_data['question_id']}"
        doc.add_paragraph(title, style='Heading2')
        doc.add_paragraph(f"æ­£ç­”ç‡: {page_data['accuracy']}%")
        doc.add_paragraph(f"å•é¡Œæ–‡: {page_data['problem']}")

        if include_images and page_data['images']:
            for img_url in page_data['images']:
                try:
                    response = requests.get(img_url)
                    if response.status_code == 200:
                        image_stream = BytesIO(response.content)
                        doc.add_paragraph()
                        doc.add_picture(image_stream, width=Inches(2.5))
                    else:
                        doc.add_paragraph(f"ç”»åƒå–å¾—å¤±æ•—: {img_url}")
                except Exception as e:
                    doc.add_paragraph(f"ç”»åƒå–å¾—ä¸­ã‚¨ãƒ©ãƒ¼: {e}")

        doc.add_paragraph("é¸æŠè‚¢:")
        for choice in page_data['choices']:
            doc.add_paragraph(choice)
        doc.add_paragraph(f"{page_data['answer']}")
        doc.add_paragraph(f"è§£èª¬: {page_data['explanation']}")
        doc.add_paragraph("-" * 50)

    filename = f"{search_query}_search_results.docx"
    doc.save(filename)
    return filename

# Streamlit UI
st.title("ğŸ©º Medu4 å•é¡Œç•ªå·ã‹ã‚‰åé›†ãƒ„ãƒ¼ãƒ«")

uploaded_file = st.file_uploader("ğŸ“„ å•é¡Œç•ªå·ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ.txt / .csvï¼‰", type=["txt", "csv"])
include_images = st.checkbox("ğŸ–¼ï¸ ç”»åƒã‚‚å«ã‚ã‚‹", value=True)
sort_by_accuracy = st.checkbox("ğŸ“‰ æ­£ç­”ç‡ãŒä½ã„é †ã«ä¸¦ã³æ›¿ãˆã‚‹", value=False)

if uploaded_file:
    question_ids = try_read_file(uploaded_file)
    if not question_ids:
        st.stop()

    urls = generate_urls_from_ids(question_ids)

    st.write(f"{len(urls)}å€‹ã®å•é¡Œã‚’å–å¾—ã—ã¾ã™ã€‚")
    progress_bar = st.progress(0)
    pages_data = []

    for i, url in enumerate(urls):
        page_data = get_page_text(url, get_images=include_images)
        if page_data:
            pages_data.append(page_data)
        else:
            st.warning(f"âŒ URLå–å¾—å¤±æ•—: {url}")
        progress_bar.progress((i + 1) / len(urls))
        time.sleep(0.2)

    # ä¸¦ã³æ›¿ãˆ
    if sort_by_accuracy:
        pages_data.sort(key=lambda x: (x["accuracy"] if isinstance(x["accuracy"], int) else 9999))

    with st.spinner("Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­..."):
        filename = create_word_doc(pages_data, "å•é¡Œç•ªå·ãƒªã‚¹ãƒˆ", include_images=include_images)

    st.success("âœ… Wordãƒ•ã‚¡ã‚¤ãƒ«ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    with open(filename, "rb") as file:
        st.download_button("ğŸ“„ Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", file, file_name=filename)
